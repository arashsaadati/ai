import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import concurrent.futures
from tqdm import tqdm
from googletrans import Translator
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import datetime

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "fa-IR,fa;q=0.9"
}

persian_sources = [
    "https://irsafam.org",
    "https://irsafam.com",
    "https://zaban24.com/ielts",
    "https://www.ieltstoday.com",
    "https://ielts-rooz.ir",
    "https://www.ieltscity.com",
    "https://mohsen-ielts.com",
    "https://www.ieltstehran.com",
    "https://ieltsidp.com",
    "https://blog.faradars.org/ielts",
    "https://7learn.com/blog/ielts",
    "https://ili.ut.ac.ir/ielts"
]

# Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡
international_sources = [
    "https://www.ielts.org",
    "https://www.cambridgeenglish.org/exams-and-tests/ielts",
    "https://takeielts.britishcouncil.org",
    "https://ielts.idp.com"
]

# Ù…ØªØ±Ø¬Ù…
translator = Translator(service_urls=['translate.googleapis.com'])

# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„ ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„Ø§Øª
try:
    tokenizer = AutoTokenizer.from_pretrained("persiannlp/parsbert-qa-quest")
    model = AutoModelForSequenceClassification.from_pretrained("persiannlp/parsbert-qa-quest")
    question_detector = pipeline(
        "text-classification",
        model=model,
        tokenizer=tokenizer,
        device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU (-1) ÛŒØ§ GPU (0)
    )
    print("âœ… Ù…Ø¯Ù„ parsBERT Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ parsBERT: {str(e)}")
    question_detector = None

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³ÙˆØ§Ù„Ø§Øª
question_patterns = [
    r"(Ø¢ÛŒÙ„ØªØ³\s*Ú†ÛŒ[Ø³Øª]|Ú†ÛŒØ³Øª\??)",
    r"(Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø·ÙˆØ±|Ø±ÙˆØ´)\s.*\?",
    r"(Ù‡Ø²ÛŒÙ†Ù‡|Ù‚ÛŒÙ…Øª|Ù…Ø¨Ù„Øº)\s.*\?",
    r"(Ù†Ù…Ø±Ù‡|Ø§Ù…ØªÛŒØ§Ø²)\s.*\?",
    r"(Ù…Ù†Ø§Ø¨Ø¹|Ú©ØªØ§Ø¨|ØªÙ…Ø±ÛŒÙ†)\s.*\?",
    r"(ØªÙØ§ÙˆØª|ÙØ±Ù‚)\s.*\?",
    r"(Ø´Ø±Ø§ÛŒØ·|Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ)\s.*\?",
    r"(Ø¢Ù…Ø§Ø¯Ú¯ÛŒ|Ø¢Ù…ÙˆØ²Ø´)\s.*\?",
    r"(Ù…Ø¯Øª|Ø²Ù…Ø§Ù†)\s.*\?",
    r"(Ú†Ø±Ø§|Ø¹Ù„Øª|Ø¯Ù„ÛŒÙ„)\s.*\?",
    r"(Ø¢ÛŒØ§|Ø§ÛŒØ§)\s.*\?",
    r"(Ú©Ø¯Ø§Ù…|Ú†Ù‡|Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø±Ø§|Ú©ÛŒ|Ú©Ø¬Ø§)\s.*\?"
]

# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
ielts_keywords = [
    'Ø¢ÛŒÙ„ØªØ³', 'ielts', 'Ø±ÛŒØ¯ÛŒÙ†Ú¯', 'Ø±Ø§ÛŒØªÛŒÙ†Ú¯',
    'Ù„ÛŒØ³Ù†ÛŒÙ†Ú¯', 'Ø§Ø³Ù¾ÛŒÚ©ÛŒÙ†Ú¯', 'Ù†Ù…Ø±Ù‡', 'Ù…Ù‡Ø§Ø¬Ø±Øª',
    'ØªØ­ØµÛŒÙ„', 'Ø¢Ø²Ù…ÙˆÙ†', 'Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ', 'Ù…Ø§Ú˜ÙˆÙ„'
]

def is_relevant(url, text):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ø¢ÛŒÙ„ØªØ³"""
    text = text.lower()
    url_check = any(kw in url.lower() for kw in ielts_keywords)
    content_check = any(kw in text[:500] for kw in ielts_keywords)
    
    if question_detector and len(text.split()) > 10:
        try:
            result = question_detector(text[:512])  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„ Ù…Ø¯Ù„
            return result[0]['label'] == 'LABEL_1' or url_check or content_check
        except:
            pass
    
    return url_check or content_check

def translate_to_persian(text):
    """ØªØ±Ø¬Ù…Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ"""
    try:
        if text.strip():
            translated = translator.translate(text, src='en', dest='fa')
            return translated.text
        return text
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {str(e)}")
        return text

def clean_text(text):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\b(?:https?|www)\S+', '', text)
    return text.strip()

def extract_links(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·"""
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            text = a.get_text().lower()
            
            if any(kw in href or kw in text for kw in ielts_keywords):
                full_url = urljoin(url, href)
                if not any(ext in full_url for ext in ['.pdf', '.jpg', '.png']):
                    links.add(full_url)
        
        return list(links)[:10]  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² {url}: {str(e)}")
        return []

def scrape_page(url):
    """Ø§Ø³Ú©Ø±Ù¾ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
    try:
        response = requests.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø­Ø°Ù Ø¹Ù†Ø§ØµØ± ØºÛŒØ±Ù…ÙÛŒØ¯
        for tag in ['script', 'style', 'iframe', 'nav', 'footer', 'header', 
                   'form', 'button', 'input', 'select']:
            for element in soup(tag):
                element.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ
        content = []
        for tag in ['article', 'main', 'section', 'div', 'p']:
            for element in soup.find_all(tag):
                text = clean_text(element.get_text())
                if len(text.split()) > 5 and is_relevant(url, text):
                    content.append(text)
        
        return ' '.join(content)
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ ØµÙØ­Ù‡ {url}: {str(e)}")
        return ""

def is_question(text):
    """ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ Ù…Ø¯Ù„ NLP"""
    if len(text.split()) < 3:
        return False
    
    has_question_mark = 'ØŸ' in text or '?' in text
    pattern_check = any(re.search(pattern, text, re.IGNORECASE) for pattern in question_patterns)
    
    if question_detector:
        try:
            result = question_detector(text[:512])
            model_check = result[0]['label'] == 'LABEL_1'
            confidence = result[0]['score'] > 0.7
            return (model_check and confidence) or (pattern_check and has_question_mark)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„: {str(e)}")
            return pattern_check and has_question_mark
    
    return pattern_check and has_question_mark

def generate_qa(text, source_type):
    """ØªÙˆÙ„ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø®"""
    qa_pairs = []
    
    # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    sentences = [s.strip() for s in re.split(r'([ØŸ?!\.]+)', text) if s.strip()]
    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) and 
                re.match(r'^[ØŸ?!\.]+$', sentences[i+1]) else '') 
                for i in range(0, len(sentences), 2)]
    
    for i in range(len(sentences)-1):
        current = sentences[i]
        next_sent = sentences[i+1] if i+1 < len(sentences) else ""
        
        if is_question(current) and len(next_sent.split()) > 3:
            if source_type == "international":
                current = translate_to_persian(current)
                next_sent = translate_to_persian(next_sent)
            
            qa_pairs.append({
                "question": current,
                "answer": next_sent,
                "confidence": question_detector(current[:512])[0]['score'] if question_detector else 0.9
            })
    
    return qa_pairs

def process_source(url, source_type):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ù†Ø¨Ø¹"""
    links = extract_links(url)
    all_qa = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(scrape_page, link): link for link in links}
        for future in tqdm(concurrent.futures.as_completed(futures), 
                          total=len(links), 
                          desc=f"Ù¾Ø±Ø¯Ø§Ø²Ø´ {url.split('//')[1][:20]}..."):
            content = future.result()
            if content:
                all_qa.extend(generate_qa(content, source_type))
    
    return all_qa

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("\nğŸ” Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...\n")
    
    dataset = {
        "metadata": {
            "version": "2.1",
            "model": "parsBERT-QA" if question_detector else "regex-only",
            "created_at": datetime.datetime.now().isoformat(),
            "sources": persian_sources + international_sources
        },
        "data": []
    }
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ
    print("\nğŸ“š Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ...")
    for source in tqdm(persian_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ"):
        dataset["data"].extend(process_source(source, "persian"))
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    print("\nğŸŒ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ...")
    for source in tqdm(international_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ"):
        dataset["data"].extend(process_source(source, "international"))
    
    # ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ
    print("\nğŸ§¹ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø§Ù„Ø§ÛŒØ´ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    unique_qa = []
    seen = set()
    
    for item in tqdm(dataset["data"], desc="Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§"):
        key = (clean_text(item["question"]), clean_text(item["answer"]))
        if key not in seen and item.get("confidence", 1) > 0.6:
            seen.add(key)
            del item["confidence"]  # Ø­Ø°Ù ÙÛŒÙ„Ø¯ Ù…ÙˆÙ‚Øª
            unique_qa.append(item)
    
    dataset["data"] = unique_qa
    dataset["metadata"]["total_qa"] = len(unique_qa)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ
    output_file = 'ielts_ai_dataset_final.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=4)
    
    print(f"\nâœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ {len(unique_qa)} Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø® Ø¯Ø± ÙØ§ÛŒÙ„ '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    print(f"âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {dataset['metadata']['model']}")

if __name__ == "__main__":
    main()