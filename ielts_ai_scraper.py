import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import concurrent.futures
from tqdm import tqdm
from googletrans import Translator
from transformers import pipeline
from persiannlp import word_tokenize

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "fa-IR,fa;q=0.9"
}

# # Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø±
# persian_sources = [
#     "https://irsafam.org",
#     "https://irsafam.com",
#     "https://zaban24.com/ielts",
#     "https://www.ieltstoday.com",
#     "https://ielts-rooz.ir",
#     "https://www.ieltscity.com",
#     "https://mohsen-ielts.com"
# ]

# # Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡
# international_sources = [
#     "https://www.ielts.org",
#     "https://takeielts.britishcouncil.org"
# ]

# Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø±
persian_sources = [
    "https://irsafam.org",
    "https://irsafam.com",
    "https://zaban24.com/ielts",
    "https://www.ieltstoday.com",
    "https://ielts-rooz.ir",
    "https://www.ieltscity.com",
    "https://mohsen-ielts.com",
    "https://www.ieltstehran.com",
    "https://ieltsidp.com",
    "https://blog.faradars.org/ielts",
    "https://7learn.com/blog/ielts",
    "https://ili.ut.ac.ir/ielts"
]

# Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡
international_sources = [
    "https://www.ielts.org",
    "https://www.cambridgeenglish.org/exams-and-tests/ielts",
    "https://takeielts.britishcouncil.org",
    "https://ielts.idp.com"
]

# Ù…ØªØ±Ø¬Ù…
translator = Translator(service_urls=['translate.googleapis.com'])

# Ù…Ø¯Ù„ NLP Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„Ø§Øª
try:
    question_detector = pipeline(
        "text-classification",
        model="persiannlp/parsbert-qa-quest",
        tokenizer="persiannlp/parsbert-qa-quest"
    )
except:
    question_detector = None

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³ÙˆØ§Ù„Ø§Øª
question_patterns = [
    r"(Ø¢ÛŒÙ„ØªØ³\s*Ú†ÛŒ[Ø³Øª]|Ú†ÛŒØ³Øª\??)",
    r"(Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø·ÙˆØ±|Ø±ÙˆØ´)\s.*\?",
    r"(Ù‡Ø²ÛŒÙ†Ù‡|Ù‚ÛŒÙ…Øª|Ù…Ø¨Ù„Øº)\s.*\?",
    r"(Ù†Ù…Ø±Ù‡|Ø§Ù…ØªÛŒØ§Ø²)\s.*\?",
    r"(Ù…Ù†Ø§Ø¨Ø¹|Ú©ØªØ§Ø¨|ØªÙ…Ø±ÛŒÙ†)\s.*\?",
    r"(ØªÙØ§ÙˆØª|ÙØ±Ù‚)\s.*\?",
    r"(Ø´Ø±Ø§ÛŒØ·|Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ)\s.*\?",
    r"(Ø¢Ù…Ø§Ø¯Ú¯ÛŒ|Ø¢Ù…ÙˆØ²Ø´)\s.*\?",
    r"(Ù…Ø¯Øª|Ø²Ù…Ø§Ù†)\s.*\?",
    r"(Ú†Ø±Ø§|Ø¹Ù„Øª|Ø¯Ù„ÛŒÙ„)\s.*\?",
    r"(Ø¢ÛŒØ§|Ø§ÛŒØ§)\s.*\?",
    r"(Ú©Ø¯Ø§Ù…|Ú†Ù‡|Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø±Ø§|Ú©ÛŒ|Ú©Ø¬Ø§)\s.*\?"
]

# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
ielts_keywords = [
    'Ø¢ÛŒÙ„ØªØ³', 'ielts', 'Ø±ÛŒØ¯ÛŒÙ†Ú¯', 'Ø±Ø§ÛŒØªÛŒÙ†Ú¯',
    'Ù„ÛŒØ³Ù†ÛŒÙ†Ú¯', 'Ø§Ø³Ù¾ÛŒÚ©ÛŒÙ†Ú¯', 'Ù†Ù…Ø±Ù‡', 'Ù…Ù‡Ø§Ø¬Ø±Øª',
    'ØªØ­ØµÛŒÙ„', 'Ø¢Ø²Ù…ÙˆÙ†', 'Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ', 'Ù…Ø§Ú˜ÙˆÙ„'
]

def is_relevant(url, text):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ø¢ÛŒÙ„ØªØ³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² NLP"""
    text = text.lower()
    url_check = any(kw in url.lower() for kw in ielts_keywords)
    content_check = any(kw in text[:500] for kw in ielts_keywords)
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø±ØªØ¨Ø·
    if question_detector and len(text.split()) > 10:
        try:
            result = question_detector(text[:512])  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„ Ù…Ø¯Ù„
            return result[0]['label'] == 'LABEL_1' or url_check or content_check
        except:
            pass
    
    return url_check or content_check

def translate_to_persian(text):
    """ØªØ±Ø¬Ù…Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ"""
    try:
        if text.strip():
            translated = translator.translate(text, src='en', dest='fa')
            return translated.text
        return text
    except:
        return text

def clean_text(text):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ"""
    # ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
    try:
        tokens = word_tokenize(text)
        text = ' '.join(tokens)
    except:
        pass
    
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\b(?:https?|www)\S+', '', text)
    return text.strip()

def extract_links(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            text = a.get_text().lower()
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
            if (any(kw in href or kw in text for kw in ielts_keywords) or
                (question_detector and len(text.split()) > 3 and 
                 question_detector(text[:128])[0]['label'] == 'LABEL_1')):
                
                full_url = urljoin(url, href)
                if not any(ext in full_url for ext in ['.pdf', '.jpg', '.png']):
                    links.add(full_url)
        
        return list(links)[:10]  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©
    except:
        return []

def scrape_page(url):
    """Ø§Ø³Ú©Ø±Ù¾ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
    try:
        response = requests.get(url, headers=headers, timeout=20)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø­Ø°Ù Ø¹Ù†Ø§ØµØ± ØºÛŒØ±Ù…ÙÛŒØ¯ Ø¨Ø§ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
        for tag in ['script', 'style', 'iframe', 'nav', 'footer', 'header', 
                   'form', 'button', 'input', 'select']:
            for element in soup(tag):
                element.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ
        content = []
        for tag in ['article', 'main', 'section', 'div[class*="content"]', 
                   'div[class*="text"]', 'p']:
            for element in soup.select(tag) if '[' in tag else soup.find_all(tag):
                text = clean_text(element.get_text())
                if len(text.split()) > 7 and is_relevant(url, text):
                    content.append(text)
        
        return ' '.join(content)
    except:
        return ""

def is_question(text):
    """ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ Ù…Ø¯Ù„ NLP"""
    # Ø§Ø¨ØªØ¯Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…ØªÙ†
    if len(text.split()) < 3:
        return False
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ø§Ù…Øª Ø³ÙˆØ§Ù„
    has_question_mark = 'ØŸ' in text or '?' in text
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡
    pattern_check = any(re.search(pattern, text, re.IGNORECASE) for pattern in question_patterns)
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ NLP Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if question_detector:
        try:
            result = question_detector(text[:512])  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„ Ù…Ø¯Ù„
            model_check = result[0]['label'] == 'LABEL_1'
            confidence = result[0]['score'] > 0.85
            return (model_check and confidence) or (pattern_check and has_question_mark)
        except:
            pass
    
    return pattern_check and has_question_mark

def generate_qa(text, source_type):
    """ØªÙˆÙ„ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø®"""
    qa_pairs = []
    
    # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ø³Ø§Ø®ØªØ§Ø± ÙØ§Ø±Ø³ÛŒ
    sentences = [s.strip() for s in re.split(r'([ØŸ?!\.]+)', text) if s.strip()]
    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) and 
                re.match(r'^[ØŸ?!\.]+$', sentences[i+1]) else '') 
                for i in range(0, len(sentences), 2)]
    
    for i in range(len(sentences)-1):
        current = sentences[i]
        next_sent = sentences[i+1] if i+1 < len(sentences) else ""
        
        if is_question(current) and len(next_sent.split()) > 4:
            if source_type == "international":
                current = translate_to_persian(current)
                next_sent = translate_to_persian(next_sent)
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø§Ø³Ø® Ø¨Ø§ NLP
            processed_answer = clean_text(next_sent)
            
            qa_pairs.append({
                "question": current,
                "answer": processed_answer,
                "confidence": question_detector(current[:512])[0]['score'] if question_detector else 0.9
            })
    
    return qa_pairs

def process_source(url, source_type):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ù†Ø¨Ø¹"""
    links = extract_links(url)
    all_qa = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(scrape_page, link): link for link in links}
        for future in tqdm(concurrent.futures.as_completed(futures), 
                          total=len(links), 
                          desc=f"Ù¾Ø±Ø¯Ø§Ø²Ø´ {url.split('//')[1][:20]}..."):
            content = future.result()
            if content:
                all_qa.extend(generate_qa(content, source_type))
    
    return all_qa

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    print("ğŸ” Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...\n")
    
    dataset = {
        "metadata": {
            "version": "2.0",
            "model": "parsBERT-QA",
            "sources": persian_sources + international_sources
        },
        "data": []
    }
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ
    print("\nğŸ“š Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ...")
    for source in tqdm(persian_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ"):
        dataset["data"].extend(process_source(source, "persian"))
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    print("\nğŸŒ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ...")
    for source in tqdm(international_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ"):
        dataset["data"].extend(process_source(source, "international"))
    
    # ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
    print("\nğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ù…Ø¯Ù„ NLP...")
    final_qa = []
    seen = set()
    
    for item in tqdm(dataset["data"], desc="Ù¾Ø§Ù„Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"):
        # Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ
        key = (clean_text(item["question"]), clean_text(item["answer"]))
        if key not in seen:
            seen.add(key)
            
            # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù‚Ø·Ø¹ÛŒØª Ù…Ø¯Ù„
            if item.get("confidence", 1) > 0.7:
                del item["confidence"]  # Ø­Ø°Ù ÙÛŒÙ„Ø¯ Ù…ÙˆÙ‚Øª
                final_qa.append(item)
    
    dataset["data"] = final_qa
    dataset["metadata"]["total_qa"] = len(final_qa)
    dataset["metadata"]["processed_at"] = datetime.datetime.now().isoformat()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ
    output_file = 'ielts_ai_dataset.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=4)
    
    print(f"\nâœ… Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ {len(final_qa)} Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø® Ø¯Ø± ÙØ§ÛŒÙ„ '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    print("ğŸ¯ Ø¯Ù‚Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ parsBERT-QA")

if __name__ == "__main__":
    import datetime
    main()