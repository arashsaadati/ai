import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import concurrent.futures
from tqdm import tqdm
from googletrans import Translator
from transformers import pipeline
import datetime

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "fa-IR,fa;q=0.9,en;q=0.8"
}

# Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø±
persian_sources = [
    "https://irsafam.org",
    "https://zaban24.com/ielts",
    "https://ielts-rooz.ir",
    "https://www.ieltscity.com"
]

# Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
international_sources = [
    "https://www.ielts.org",
    "https://takeielts.britishcouncil.org"
]

# Ù…ØªØ±Ø¬Ù…
translator = Translator(service_urls=['translate.googleapis.com'])

# Ù…Ø¯Ù„ Ø³Ø¨Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„Ø§Øª (DistilBERT)
try:
    question_detector = pipeline(
        "text-classification",
        model="distilbert-base-uncased",  # Ù…Ø¯Ù„ Ø³Ø¨Ú©â€ŒØªØ±
        tokenizer="distilbert-base-uncased",
        device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU
    )
    print("âœ… Ù…Ø¯Ù„ DistilBERT Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {str(e)}")
    question_detector = None

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø³ÙˆØ§Ù„Ø§Øª (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡)
question_patterns = [
    r"(Ø¢ÛŒÙ„ØªØ³\s*Ú†ÛŒ[Ø³Øª]|Ú†ÛŒØ³Øª\??)",
    r"(Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø·ÙˆØ±|Ø±ÙˆØ´)\s.*\?",
    r"(Ù‡Ø²ÛŒÙ†Ù‡|Ù‚ÛŒÙ…Øª|Ù…Ø¨Ù„Øº)\s.*\?",
    r"(Ù†Ù…Ø±Ù‡|Ø§Ù…ØªÛŒØ§Ø²)\s.*\?",
    r"(Ù…Ù†Ø§Ø¨Ø¹|Ú©ØªØ§Ø¨|ØªÙ…Ø±ÛŒÙ†)\s.*\?",
    r"(ØªÙØ§ÙˆØª|ÙØ±Ù‚)\s.*\?",
    r"(Ø´Ø±Ø§ÛŒØ·|Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ)\s.*\?",
    r"(Ø¢Ù…Ø§Ø¯Ú¯ÛŒ|Ø¢Ù…ÙˆØ²Ø´)\s.*\?",
    r"(Ù…Ø¯Øª|Ø²Ù…Ø§Ù†)\s.*\?",
    r"(Ú†Ø±Ø§|Ø¹Ù„Øª|Ø¯Ù„ÛŒÙ„)\s.*\?",
    r"(Ø¢ÛŒØ§|Ø§ÛŒØ§)\s.*\?",
    r"(Ú©Ø¯Ø§Ù…|Ú†Ù‡|Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø±Ø§|Ú©ÛŒ|Ú©Ø¬Ø§)\s.*\?"
]

# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
ielts_keywords = [
    'Ø¢ÛŒÙ„ØªØ³', 'ielts', 'Ø±ÛŒØ¯ÛŒÙ†Ú¯', 'Ø±Ø§ÛŒØªÛŒÙ†Ú¯',
    'Ù„ÛŒØ³Ù†ÛŒÙ†Ú¯', 'Ø§Ø³Ù¾ÛŒÚ©ÛŒÙ†Ú¯', 'Ù†Ù…Ø±Ù‡', 'Ù…Ù‡Ø§Ø¬Ø±Øª',
    'ØªØ­ØµÛŒÙ„', 'Ø¢Ø²Ù…ÙˆÙ†', 'Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ'
]

def is_relevant(url, text):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ø¢ÛŒÙ„ØªØ³"""
    text = text.lower()
    url_check = any(kw in url.lower() for kw in ielts_keywords)
    content_check = any(kw in text[:300] for kw in ielts_keywords)
    return url_check or content_check

def clean_text(text):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\b(?:https?|www)\S+', '', text)
    return text.strip()

def extract_links(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·"""
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            text = a.get_text().lower()
            if any(kw in href or kw in text for kw in ielts_keywords):
                full_url = urljoin(url, href)
                if not any(ext in full_url for ext in ['.pdf', '.jpg', '.png']):
                    links.add(full_url)
        return list(links)[:15]  # Ø­Ø¯Ø§Ú©Ø«Ø± 15 Ù„ÛŒÙ†Ú©
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© Ø§Ø² {url}: {str(e)}")
        return []

def scrape_page(url):
    """Ø§Ø³Ú©Ø±Ù¾ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡"""
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø­Ø°Ù Ø¹Ù†Ø§ØµØ± ØºÛŒØ±Ù…ÙÛŒØ¯
        for tag in ['script', 'style', 'iframe', 'nav', 'footer', 'header']:
            for element in soup(tag):
                element.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ
        content = []
        for tag in ['p', 'div', 'article', 'section']:
            for element in soup.find_all(tag):
                text = clean_text(element.get_text())
                if len(text.split()) > 5 and is_relevant(url, text):
                    content.append(text)
        
        return ' '.join(content)
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ {url}: {str(e)}")
        return ""

def is_question(text):
    """ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ Ù…Ø¯Ù„"""
    if len(text.split()) < 3:
        return False
    
    has_question_mark = 'ØŸ' in text or '?' in text
    pattern_check = any(re.search(pattern, text, re.IGNORECASE) for pattern in question_patterns)
    
    if question_detector and len(text) > 10:
        try:
            result = question_detector(text[:512])
            return (result[0]['label'] == 'LABEL_1' and result[0]['score'] > 0.7) or pattern_check
        except:
            return pattern_check and has_question_mark
    
    return pattern_check and has_question_mark

def generate_qa(text, source_type):
    """ØªÙˆÙ„ÛŒØ¯ Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø®"""
    qa_pairs = []
    sentences = [s.strip() for s in re.split(r'([ØŸ?!\.])', text) if s.strip()]
    sentences = [s + sentences[i+1] if i+1 < len(sentences) and re.match(r'^[ØŸ?!\.]$', sentences[i+1]) else s 
               for i, s in enumerate(sentences) if not re.match(r'^[ØŸ?!\.]$', s)]
    
    for i in range(len(sentences)-1):
        current = sentences[i]
        next_sent = sentences[i+1] if i+1 < len(sentences) else ""
        
        if is_question(current) and len(next_sent.split()) > 3:
            if source_type == "international":
                current = translate_to_persian(current)
                next_sent = translate_to_persian(next_sent)
            
            qa_pairs.append({
                "question": current,
                "answer": next_sent
            })
    
    return qa_pairs

def process_source(url, source_type):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù…Ù†Ø¨Ø¹"""
    links = extract_links(url)
    all_qa = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(scrape_page, link): link for link in links}
        for future in tqdm(concurrent.futures.as_completed(futures), 
                         total=len(links), 
                         desc=f"Ù¾Ø±Ø¯Ø§Ø²Ø´ {url.split('//')[1][:15]}..."):
            content = future.result()
            if content:
                all_qa.extend(generate_qa(content, source_type))
    
    return all_qa

def main():
    print("\nğŸ” Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ„ØªØ³...")
    
    dataset = {
        "metadata": {
            "version": "2.2",
            "model": "DistilBERT" if question_detector else "Regex",
            "created_at": datetime.datetime.now().isoformat(),
            "sources": persian_sources + international_sources
        },
        "data": []
    }
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ
    print("\nğŸ“š Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ...")
    for source in tqdm(persian_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ ÙØ§Ø±Ø³ÛŒ"):
        dataset["data"].extend(process_source(source, "persian"))
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ
    print("\nğŸŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ...")
    for source in tqdm(international_sources, desc="Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ"):
        dataset["data"].extend(process_source(source, "international"))
    
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
    unique_qa = []
    seen = set()
    for item in dataset["data"]:
        key = (item["question"][:100], item["answer"][:100])  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ø²ÛŒØ§Ø¯
        if key not in seen:
            seen.add(key)
            unique_qa.append(item)
    
    dataset["data"] = unique_qa
    dataset["metadata"]["total_qa"] = len(unique_qa)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª
    output_file = "ielts_dataset_light.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=4)
    
    print(f"\nâœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ {len(unique_qa)} Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø® Ø¯Ø± '{output_file}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    print(f"âš¡ Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {dataset['metadata']['model']}")

if __name__ == "__main__":
    main()