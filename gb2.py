import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import concurrent.futures
from tqdm import tqdm
from googletrans import Translator
from transformers import pipeline
import datetime

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯
class Config:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "fa-IR,fa;q=0.9,en;q=0.8",
            "Referer": "https://www.google.com/"
        }
        
        # Ù…Ù†Ø§Ø¨Ø¹ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
        self.primary_sources = {
            "persian": [
                "https://irsafam.org",
                "https://zaban24.com/ielts",
                "https://ielts-rooz.ir"
            ],
            "international": [
                "https://www.ielts.org",
                "https://takeielts.britishcouncil.org"
            ]
        }
        
        self.backup_sources = {
            "persian": [
                "https://fa.wikipedia.org/wiki/Ø¢ÛŒÙ„ØªØ³",
                "https://7learn.com/blog/ielts",
                "https://blog.faradars.org/ielts"
            ],
            "international": [
                "https://ielts.idp.com",
                "https://www.cambridgeenglish.org/exams-and-tests/ielts"
            ]
        }
        
        self.current_sources = self.primary_sources.copy()
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡
        self.question_patterns = [
            r"(Ø¢ÛŒÙ„ØªØ³|ØªØ§ÙÙ„|PTE)\s*Ú†ÛŒ[Ø³Øª]?\??",
            r"(Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø·ÙˆØ±|Ø±ÙˆØ´|Ø·Ø±ÛŒÙ‚Ù‡|Ù…Ø±Ø§Ø­Ù„)\s.*\??",
            r"(Ù‡Ø²ÛŒÙ†Ù‡|Ù‚ÛŒÙ…Øª|Ù…Ø¨Ù„Øº|Ø´Ù‡Ø±ÛŒÙ‡)\s.*\??",
            r"(Ù†Ù…Ø±Ù‡|Ø§Ù…ØªÛŒØ§Ø²|Ø¨Ø§Ù†Ø¯|Ù†ØªÛŒØ¬Ù‡)\s.*\??",
            r"(Ù…Ù†Ø§Ø¨Ø¹|Ú©ØªØ§Ø¨|Ø¬Ø²ÙˆÙ‡|Ù†Ù…ÙˆÙ†Ù‡ Ø³ÙˆØ§Ù„|ØªÙ…Ø±ÛŒÙ†)\s.*\??",
            r"(ØªÙØ§ÙˆØª|ÙØ±Ù‚|Ù…Ù‚Ø§ÛŒØ³Ù‡|Ø¨Ø±ØªØ±ÛŒ)\s.*\??",
            r"(Ø´Ø±Ø§ÛŒØ·|Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ|Ù¾ÛŒØ´ Ù†ÛŒØ§Ø²|Ø§Ù„Ø²Ø§Ù…Ø§Øª)\s.*\??",
            r"(Ø¢Ù…Ø§Ø¯Ú¯ÛŒ|Ø¢Ù…ÙˆØ²Ø´|ØªÙ…Ø±ÛŒÙ†|Ú©Ù„Ø§Ø³)\s.*\??",
            r"(Ù…Ø¯Øª|Ø²Ù…Ø§Ù†|Ø·ÙˆÙ„ Ø¯ÙˆØ±Ù‡|ØªØ§Ø±ÛŒØ®)\s.*\??",
            r"(Ú†Ø±Ø§|Ø¹Ù„Øª|Ø¯Ù„ÛŒÙ„|Ù…Ù†Ø¸ÙˆØ±)\s.*\??",
            r"(Ø¢ÛŒØ§|Ø§ÛŒØ§|Ù‡Ù…ÛŒÙ†Ø·ÙˆØ±|Ù…ÛŒØ´Ù‡)\s.*\??",
            r"(Ú©Ø¯Ø§Ù…|Ú†Ù‡|Ú†Ú¯ÙˆÙ†Ù‡|Ú†Ø±Ø§|Ú©ÛŒ|Ú©Ø¬Ø§|Ú†Ù‚Ø¯Ø±)\s.*\??",
            r".*\?(ØŸ|\?|$)"
        ]
        
        self.keywords = [
            'Ø¢ÛŒÙ„ØªØ³', 'ielts', 'Ø±ÛŒØ¯ÛŒÙ†Ú¯', 'Ø±Ø§ÛŒØªÛŒÙ†Ú¯',
            'Ù„ÛŒØ³Ù†ÛŒÙ†Ú¯', 'Ø§Ø³Ù¾ÛŒÚ©ÛŒÙ†Ú¯', 'Ù†Ù…Ø±Ù‡', 'Ù…Ù‡Ø§Ø¬Ø±Øª',
            'ØªØ­ØµÛŒÙ„', 'Ø¢Ø²Ù…ÙˆÙ†', 'Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ', 'Ù…Ø§Ú˜ÙˆÙ„'
        ]
        
        self.setup_translator()
        self.setup_nlp_model()
    
    def setup_translator(self):
        """ØªÙ†Ø¸ÛŒÙ… Ù…ØªØ±Ø¬Ù… Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª fallback"""
        try:
            self.translator = Translator(service_urls=['translate.googleapis.com'])
            test_trans = self.translator.translate("hello", src='en', dest='fa').text
            assert test_trans == "Ø³Ù„Ø§Ù…"
        except:
            print("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ØªØ±Ø¬Ù…ØŒ ØªØ±Ø¬Ù…Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø´Ø¯")
            self.translator = None
    
    def setup_nlp_model(self):
        """ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ NLP Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª fallback"""
        try:
            self.question_detector = pipeline(
                "text-classification",
                model="HooshvareLab/bert-fa-base-uncased",
                device=-1
            )
            test_pred = self.question_detector("Ù‡Ø²ÛŒÙ†Ù‡ Ø¢Ø²Ù…ÙˆÙ† Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ")[0]
            assert test_pred['label'] == 'LABEL_1'
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ NLP: {str(e)}")
            self.question_detector = None
    
    def switch_to_backup(self):
        """Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†"""
        print("ğŸ” ØªØ¹ÙˆÛŒØ¶ Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†...")
        self.current_sources = self.backup_sources.copy()

config = Config()

# ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø®Ø·Ø§
def smart_request(url, max_retries=3):
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
    for i in range(max_retries):
        try:
            response = requests.get(url, headers=config.headers, timeout=15)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª {url} (ØªÙ„Ø§Ø´ {i+1}/{max_retries}): {str(e)}")
            if i == max_retries - 1:
                return None

def extract_links(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
    try:
        response = smart_request(url)
        if not response:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            text = a.get_text().lower()
            
            if (any(kw in href or kw in text for kw in config.keywords) or
                (config.question_detector and len(text.split()) > 3 and 
                 config.question_detector(text[:100])[0]['label'] == 'LABEL_1')):
                
                full_url = urljoin(url, href)
                if not any(ext in full_url for ext in ['.pdf', '.jpg', '.png', '.docx']):
                    links.add(full_url)
        
        return list(links)[:15]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© Ø§Ø² {url}: {str(e)}")
        return []

def clean_text(text):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ù†Ø¹Ø·Ø§Ù Ø¨ÛŒØ´ØªØ±"""
    if not text:
        return ""
        
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[.*?\]|\(.*?\)|\{.*?\}', '', text)
    text = re.sub(r'\b(?:https?|www|ftp)\S+', '', text)
    text = re.sub(r'[^\w\s.,;!?ØŸØŒ]', '', text)
    return text.strip()

def is_question(text):
    """ØªØ´Ø®ÛŒØµ Ø³ÙˆØ§Ù„ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø±ÙˆØ´â€ŒÙ‡Ø§"""
    if not text or len(text.split()) < 3:
        return False
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø¬Ú©Ø³
    pattern_match = any(re.search(pattern, text, re.IGNORECASE) for pattern in config.question_patterns)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ NLP Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
    if config.question_detector:
        try:
            result = config.question_detector(text[:512])[0]
            model_match = result['label'] == 'LABEL_1' and result['score'] > 0.6
        except:
            model_match = False
    else:
        model_match = False
    
    # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
    return (pattern_match or model_match) and ('ØŸ' in text or '?' in text)

def generate_qa(text, source_type):
    """ØªÙˆÙ„ÛŒØ¯ Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ù†Ø¹Ø·Ø§Ù"""
    if not text:
        return []
    
    qa_pairs = []
    sentences = [s.strip() for s in re.split(r'([ØŸ?!.])', text) if s.strip()]
    sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) and 
                re.match(r'^[ØŸ?!.]$', sentences[i+1]) else '') 
                for i in range(0, len(sentences), 2)]
    
    for i in range(len(sentences)-1):
        current = sentences[i]
        next_sent = sentences[i+1] if i+1 < len(sentences) else ""
        
        if is_question(current) and len(next_sent.split()) > 2:
            if source_type == "international" and config.translator:
                try:
                    current = config.translator.translate(current, src='en', dest='fa').text
                    next_sent = config.translator.translate(next_sent, src='en', dest='fa').text
                except:
                    pass
            
            qa_pairs.append({
                "question": clean_text(current),
                "answer": clean_text(next_sent)
            })
    
    return qa_pairs

def process_source(url, source_type):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÛŒÚ© Ù…Ù†Ø¨Ø¹"""
    try:
        links = extract_links(url)
        if not links:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± {url} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return []
        
        all_qa = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(smart_request, link): link for link in links}
            for future in tqdm(concurrent.futures.as_completed(futures), 
                             total=len(links), 
                             desc=f"Ù¾Ø±Ø¯Ø§Ø²Ø´ {url[:30]}..."):
                link = futures[future]
                try:
                    response = future.result()
                    if response:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        content = ' '.join([clean_text(p.get_text()) 
                                          for p in soup.find_all(['p', 'div', 'article']) 
                                          if p.get_text().strip()])
                        all_qa.extend(generate_qa(content, source_type))
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {link}: {str(e)}")
        
        return all_qa
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø´Ø¯ÛŒØ¯ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {url}: {str(e)}")
        return []

def main():
    print("\nğŸ” Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ„ØªØ³ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯\n")
    print(f"âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ:")
    print(f"- Ù…ØªØ±Ø¬Ù…: {'ÙØ¹Ø§Ù„' if config.translator else 'ØºÛŒØ±ÙØ¹Ø§Ù„'}")
    print(f"- Ù…Ø¯Ù„ NLP: {'ÙØ¹Ø§Ù„' if config.question_detector else 'ØºÛŒØ±ÙØ¹Ø§Ù„'}")
    
    dataset = {
        "metadata": {
            "version": "3.0",
            "model": "bert-fa" if config.question_detector else "regex",
            "created_at": datetime.datetime.now().isoformat(),
            "sources": config.current_sources
        },
        "data": []
    }
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹
    for lang, sources in config.current_sources.items():
        print(f"\nğŸŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø§Ø¨Ø¹ {lang}...")
        for source in sources:
            qa_pairs = process_source(source, lang)
            if not qa_pairs and source in config.primary_sources[lang]:
                print(f"âš ï¸ Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ {source} Ù¾Ø§Ø³Ø® Ù†Ø¯Ø§Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù¾Ø´ØªÛŒØ¨Ø§Ù†...")
                config.switch_to_backup()
                qa_pairs = process_source(source, lang)
                
            dataset["data"].extend(qa_pairs)
            print(f"âœ… {len(qa_pairs)} Ø³ÙˆØ§Ù„ Ø§Ø² {source} Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
    unique_qa = []
    seen = set()
    for item in dataset["data"]:
        key = (item["question"][:100], item["answer"][:100])
        if key not in seen:
            seen.add(key)
            unique_qa.append(item)
    
    dataset["data"] = unique_qa
    dataset["metadata"]["total_qa"] = len(unique_qa)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª
    output_file = "ielts_smart_dataset.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=4)
    
    print(f"\nğŸ‰ Ø¯ÛŒØªØ§Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø§ {len(unique_qa)} Ø³ÙˆØ§Ù„-Ù¾Ø§Ø³Ø® Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
    print(f"- ÙØ§ÛŒÙ„: {output_file}")
    print(f"- Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {dataset['metadata']['model']}")
    print(f"- Ù…Ù†Ø§Ø¨Ø¹: {len(dataset['metadata']['sources']['persian'])} ÙØ§Ø±Ø³ÛŒ, {len(dataset['metadata']['sources']['international'])} Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ")

if __name__ == "__main__":
    main()